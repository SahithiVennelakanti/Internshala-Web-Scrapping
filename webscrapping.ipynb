{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c09a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/data-science-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f65c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/machine-learning-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df1=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09970f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/python-django,web-development-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df2=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bddbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/android-app-development-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df3=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ffa21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/cloud-computing-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df4=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddfd4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/analytics,front-end-development,full-stack-development,java,javascript-development,software-development,software-testing-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df5=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b72f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=pd.concat([df,df1,df2,df3,df4,df5],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2979f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tdf.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f6b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=tdf.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2976b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6=pd.read_csv(r\"C:\\Users\\SAHITHI V\\Desktop\\pandas\\Untitled Folder\\combinedinternships1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0128be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7=pd.concat([tdf,df6],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853ec898",
   "metadata": {},
   "outputs": [],
   "source": [
    "df7=(df7.drop_duplicates())  # combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2de3c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/software-development,software-testing-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df8=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a5ebfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/javascript-development,node-js-development-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df9=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcfcf806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/computer-vision-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df10=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25899ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/java,ui-ux-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df11=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3f51c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/artificial-intelligence-ai-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df12=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "172a7eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/database-building-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df13=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f3ef451",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/cyber-security-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df14=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72041b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc5476cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page=requests.get(\"https://internshala.com/internships/video-making-editing-internship\")\n",
    "soup=BeautifulSoup(page.text)\n",
    "all_jobs_links=soup.find_all(\"h3\",class_=\"job-internship-name\")\n",
    "len(all_jobs_links)\n",
    "base_url=\"https://internshala.com\"\n",
    "job_detail_urls=[]\n",
    "for i in all_jobs_links:\n",
    "    a_tag = i.find(\"a\", class_=\"job-title-href\")\n",
    "    if a_tag and a_tag.get(\"href\"):\n",
    "        full_url = base_url + a_tag[\"href\"]\n",
    "        job_detail_urls.append(full_url)\n",
    "\n",
    "l1=[]\n",
    "d1=[]\n",
    "s1=[]\n",
    "t1=[]\n",
    "c1=[]\n",
    "o1=[]\n",
    "ap=[]\n",
    "tp=[]\n",
    "th=[]\n",
    "sw=[]\n",
    "soup=[]\n",
    "\n",
    "\n",
    "for i in job_detail_urls:\n",
    "    page=requests.get(i)\n",
    "    soup.append(BeautifulSoup(page.text))\n",
    "\n",
    "for i in soup:\n",
    "    t1.append(i.find_all(\"div\",class_=\"heading_4_5 profile\")[0].text.strip())\n",
    "    c1.append(i.find_all(\"a\",class_=\"link_display_like_text\")[0].text.strip())\n",
    "    l1.append(i.find_all(\"div\",id=\"location_names\")[0].text.strip())\n",
    "    s1.append(i.find_all(\"span\",class_=\"stipend\")[0].text.strip())\n",
    "    k=i.find_all(\"div\",class_=\"item_body\")\n",
    "    for j in range(len(k)):\n",
    "        if j==1:\n",
    "            d1.append(k[j].text.strip())\n",
    "    ap.append(i.find_all(\"div\",class_=\"applications_message\")[0].text.strip())\n",
    "    \n",
    "    \n",
    "    n = i.find_all(\"div\", class_=\"text body-main\")\n",
    "    sw_val = tp_val = th_val = \"N/A\"\n",
    "    \n",
    "    if len(n) > 0:\n",
    "        sw_val = n[0].text.strip()\n",
    "    if len(n) > 1:\n",
    "        tp_val = n[1].text.strip()\n",
    "    if len(n) > 2:\n",
    "        th_val = n[2].text.strip()\n",
    "\n",
    "    sw.append(sw_val)\n",
    "    tp.append(tp_val)\n",
    "    th.append(th_val)\n",
    "            \n",
    "            \n",
    "    \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df15=pd.DataFrame({\"CompanyName\":c1,\"Title\":t1,\"location\":l1,\"duration\":d1,\"salary\":s1,\"No_of_applied\":ap,\"Total_postings\":tp,\"Total_hired\":th,\"Started_since\":sw})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c027fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=pd.concat([df7,df8,df9,df10,df11,df12,df13,df14,df15],ignore_index=True).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9411a802",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CompanyName', 'Title', 'location', 'duration', 'salary',\n",
       "       'No_of_applied', 'Total_postings', 'Total_hired', 'Started_since',\n",
       "       'Unnamed: 0.1', 'Unnamed: 0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ba6015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf=tdf.drop(['Unnamed: 0.1', 'Unnamed: 0'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b765440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.to_csv(\"c2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c831edd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
